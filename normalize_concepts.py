import json
import re
import time
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv

load_dotenv()

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)


def normalize_concepts(text, raw_concepts):
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia ph√°p lu·∫≠t. Nhi·ªám v·ª• c·ªßa b·∫°n l√† chu·∫©n h√≥a danh s√°ch keyphrase ph√°p l√Ω.

    D∆∞·ªõi ƒë√¢y l√†:
    - VƒÉn b·∫£n lu·∫≠t (m·ªôt ƒëi·ªÅu)
    - Danh s√°ch keyphrase ·ª©ng vi√™n t·ª´ h·ªá th·ªëng ph√¢n t√≠ch c√∫ ph√°p

    Y√™u c·∫ßu:
    1. Chu·∫©n h√≥a l·∫°i danh s√°ch keyphrase th√†nh c√°c c·ª•m danh t·ª´ ph√°p l√Ω r√µ nghƒ©a.
    2. Vi·∫øt theo ƒë·ªãnh d·∫°ng b√¨nh th∆∞·ªùng, kh√¥ng vi·∫øt hoa.
    3. Lo·∫°i b·ªè nh·ªØng c·ª•m kh√¥ng mang √Ω nghƒ©a ph√°p l√Ω r√µ r√†ng.
    4. Gi·ªØ l·∫°i v√† b·ªï sung c√°c thu·∫≠t ng·ªØ ƒë∆°n l·∫ª quan tr·ªçng th·ªÉ hi·ªán ch·ªß th·ªÉ ph√°p lu·∫≠t, c∆° quan nh√† n∆∞·ªõc, t·ªï ch·ª©c, v√≠ d·ª• nh∆∞ "ch√≠nh ph·ªß", "b·ªô", "t√≤a √°n", "c∆° quan".
    5. B·ªï sung keyphrase quan tr·ªçng n·∫øu b·ªã thi·∫øu, d·ª±a tr√™n n·ªôi dung vƒÉn b·∫£n.

    Tr·∫£ v·ªÅ k·∫øt qu·∫£ duy nh·∫•t l√† m·ªôt danh s√°ch JSON Python c√°c chu·ªói.

    ### VƒÉn b·∫£n:
    {text}

    ### Keyphrase ·ª©ng vi√™n:
    {raw_concepts}
    """

    print(prompt)
    response = llm.invoke(prompt)
    response_text = response.content
    cleaned = re.sub(r"^```json\s*|\s*```$", "", response_text.strip())
    print(cleaned)
    try:
        return json.loads(cleaned)
    except Exception as e:
        raise ValueError(f"L·ªói x·ª≠ l√Ω ph·∫£n h·ªìi: {str(e)}\nN·ªôi dung tr·∫£ v·ªÅ: {response_text}")


def normalize_all_concepts(input_path: str, output_path: str = "normalized_concepts.json", rpm: int = 15):
    with open(input_path, "r", encoding="utf-8") as f:
        articles = json.load(f)

    results = []
    timestamps = []

    for i, article in enumerate(articles):
        article_id = article["article_id"]
        article_number = article.get("article_number", "unknown")
        article_text = article["text"]
        raw_concepts = [c["name"] for c in article.get("concepts", [])]

        if not raw_concepts:
            continue

        print(f"üîç Chu·∫©n h√≥a concepts t·ª´ ƒêi·ªÅu {article_number} ({i + 1}/{len(articles)})")

        try:
            normalized = normalize_concepts(article_text, raw_concepts)

            result = {
                "article_id": article_id,
                "article_number": article_number,
                "normalized_concepts": normalized
            }
        except Exception as e:
            result = {
                "article_id": article_id,
                "article_number": article_number,
                "error": str(e)
            }

        results.append(result)

        with open(output_path, "w", encoding="utf-8") as out_f:
            json.dump(results, out_f, ensure_ascii=False, indent=2)

        # Rate limiting
        timestamps = [t for t in timestamps if time.time() - t < 60]
        timestamps.append(time.time())
        if len(timestamps) >= rpm:
            wait = 60 - (time.time() - timestamps[0])
            print(f"‚è≥ ƒê·∫°t gi·ªõi h·∫°n API, ch·ªù {wait:.2f} gi√¢y...")
            time.sleep(wait)

    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(results)} ƒëi·ªÅu. K·∫øt qu·∫£ l∆∞u t·∫°i: {output_path}")
    return results


if __name__ == "__main__":
    normalize_all_concepts("article_concepts_vncore.json", "normalized_concepts.json", rpm=15)
